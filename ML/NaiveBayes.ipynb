{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "356b2a37-6b5a-4496-83e9-a29063263fa7",
   "metadata": {},
   "source": [
    "### Bayes Theorem\n",
    "## P(A|B) = P(B|A) X P(A) / P(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aaef08-84e0-40c2-82a3-8c5dc73a333b",
   "metadata": {},
   "source": [
    "# If we know the prior distribution and the likelihood (as observed from the data), we can normalize the product of these two to yield a posterior estimate of A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f195101-f6bf-4058-a4f9-90c2905b26fd",
   "metadata": {},
   "source": [
    "### However, this only works if we assume that A_i is iid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ace428ab-0af7-4cdf-ad7c-30e82b7d004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5e8b2889-f2a0-4c82-b99c-73cfba41857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "class NB:\n",
    "    def __init__(self, classes:[str]):\n",
    "        # A d[word_k] = (neg_count, pos_count)\n",
    "        # This dictionary will store and update the priors and likelihoods\n",
    "        \n",
    "        # Notice we initialize with a small number greater than 0. We will do this for all\n",
    "        # counts, to avoid unencountered classes and words\n",
    "        self.classes = classes\n",
    "        self.prior_counts = dict(zip(classes, [0.1]*len(classes)))\n",
    "        self.ll_counts = {}\n",
    "        \n",
    "    def get_prior(self, label) -> float:\n",
    "        '''\n",
    "        get_prior will produce the current prior probability of a given label\n",
    "        '''\n",
    "        return self.prior_counts[label] / sum(self.prior_counts.values())\n",
    "        \n",
    "    def get_individual_likelihood(self, feature, label) -> float:\n",
    "        # if the given feature has not yet been seen, update the dictionary to include it\n",
    "        if feature not in self.ll_counts.keys():\n",
    "            self.ll_counts[feature] = dict(zip(self.classes, [0.1]*len(self.classes)))\n",
    "            self.ll_counts[feature][label] +=1\n",
    "        return self.ll_counts[feature][label]/sum(self.ll_counts[feature].values())\n",
    "        \n",
    "    def get_likelihood(self, label, features):\n",
    "        '''\n",
    "        the total likelihood is the product of all features given a label\n",
    "        '''\n",
    "        ll = 1\n",
    "        for f in features:\n",
    "            ll *=self.get_individual_likelihood(f.lower(), label)\n",
    "        return ll\n",
    "        \n",
    "    def get_posterior(self, label, features):\n",
    "        posterior = self.get_prior(label) * self.get_likelihood(label, features)\n",
    "        total = sum([self.get_prior(l) * self.get_likelihood(l, features) for l in self.classes])\n",
    "        return posterior / total\n",
    "        \n",
    "    def fit(self, features, label):\n",
    "        ''' \n",
    "        fit(features, label) will update the model using a streaming line of features (i.e. words) and label\n",
    "        '''\n",
    "        # Update the prior counts\n",
    "        self.prior_counts[label] += 1\n",
    "        for f in features:\n",
    "            # Add some feature processing here\n",
    "            f = f.lower()\n",
    "            if f not in self.ll_counts.keys():\n",
    "                self.ll_counts[f] = dict(zip(self.classes, [0.1]*len(self.classes)))\n",
    "            self.ll_counts[f][label] +=1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "542706c5-7cc0-49e3-bdfc-e8565f187c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NB(['pos', 'neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8cfe0698-c562-4426-a024-37a2b4a0748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(['yum', 'delicious'], 'pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7d5d0612-1c95-4f2c-bf61-d410ee922ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(['is', 'not', 'great'], 'neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0f0a75b4-914a-4879-9e94-aa39531a3824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'yum': {'pos': 1.1, 'neg': 0.1},\n",
       " 'delicious': {'pos': 1.1, 'neg': 0.1},\n",
       " 'is': {'pos': 0.1, 'neg': 1.1},\n",
       " 'not': {'pos': 0.1, 'neg': 1.1},\n",
       " 'great': {'pos': 0.1, 'neg': 1.1}}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.ll_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "43a04ada-beb8-40b8-b1f4-568bb323efd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_posterior('pos', ['food', 'is', 'delicious'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "be2992f9-3b2f-4554-9b5c-9b5943fe2464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg/cv000_29416'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.fileids()[0].replace('.txt', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d9e8a752-fada-4faf-9ed5-aea9bd476c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NB(['pos', 'neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "7d3c844b-18a8-4f90-996b-21c9730a78ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a83461d9-4859-4e4c-a893-d775b9160b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = Pipeline([\n",
    "    ('EncodeText', CountVectorizer()),\n",
    "    ('NaiveBayes', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "5c8b8019-4bf8-49e2-90f7-1765bfeb92af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x3 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorizer().fit_transform('this is me'.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "19630bd6-ef49-4bec-b6ff-18aa2c97c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "3bb79595-34fc-4ba7-9d71-e1296898f345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x5 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 5 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorizer().fit_transform(['this', 'is', 'me','boo', 'you'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "2745d1ef-47cc-437a-bc35-e82f02f4ea53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-188-93ec4a7e0c3e>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  nb.fit(np.array([['this', 'is', 'me'], ['boo', 'you']]), ['pos', 'neg'])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-188-93ec4a7e0c3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'this'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'me'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'boo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'you'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'pos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'neg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/jupyter_38/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \"\"\"\n\u001b[1;32m    340\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    343\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[0;32m~/anaconda3/envs/jupyter_38/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    301\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;31m# Fit or load from cache the current transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    304\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jupyter_38/lib/python3.8/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jupyter_38/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jupyter_38/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1202\u001b[0;31m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[1;32m   1203\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jupyter_38/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jupyter_38/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jupyter_38/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \"\"\"\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "nb.fit(np.array([['this', 'is', 'me'], ['boo', 'you']]), ['pos', 'neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "60eaddc6-9d0f-495f-b63e-91f0c284407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = BernoulliNB(alpha = 0.1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ea282c5b-33ff-4b93-be2d-865c5c47999e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pos': 0.1, 'neg': 0.1}\n",
      "{'pos': 0.1, 'neg': 10.1}\n",
      "{'pos': 0.1, 'neg': 20.1}\n",
      "{'pos': 0.1, 'neg': 30.1}\n",
      "{'pos': 0.1, 'neg': 40.1}\n",
      "{'pos': 0.1, 'neg': 50.1}\n",
      "{'pos': 0.1, 'neg': 60.1}\n",
      "{'pos': 0.1, 'neg': 70.1}\n",
      "{'pos': 0.1, 'neg': 80.1}\n",
      "{'pos': 0.1, 'neg': 90.1}\n",
      "{'pos': 0.1, 'neg': 100.1}\n",
      "{'pos': 0.1, 'neg': 110.1}\n",
      "{'pos': 0.1, 'neg': 120.1}\n",
      "{'pos': 0.1, 'neg': 130.1}\n",
      "{'pos': 0.1, 'neg': 140.1}\n",
      "{'pos': 0.1, 'neg': 150.1}\n",
      "{'pos': 0.1, 'neg': 160.1}\n",
      "{'pos': 0.1, 'neg': 170.1}\n",
      "{'pos': 0.1, 'neg': 180.1}\n",
      "{'pos': 0.1, 'neg': 190.1}\n",
      "{'pos': 0.1, 'neg': 200.1}\n",
      "{'pos': 0.1, 'neg': 210.1}\n",
      "{'pos': 0.1, 'neg': 220.1}\n",
      "{'pos': 0.1, 'neg': 230.1}\n",
      "{'pos': 0.1, 'neg': 240.1}\n",
      "{'pos': 0.1, 'neg': 250.1}\n",
      "{'pos': 0.1, 'neg': 260.1}\n",
      "{'pos': 0.1, 'neg': 270.1}\n",
      "{'pos': 0.1, 'neg': 280.1}\n",
      "{'pos': 0.1, 'neg': 290.1}\n",
      "{'pos': 0.1, 'neg': 300.1}\n",
      "{'pos': 0.1, 'neg': 310.1}\n",
      "{'pos': 0.1, 'neg': 320.1}\n",
      "{'pos': 0.1, 'neg': 330.1}\n",
      "{'pos': 0.1, 'neg': 340.1}\n",
      "{'pos': 0.1, 'neg': 350.1}\n",
      "{'pos': 0.1, 'neg': 360.1}\n",
      "{'pos': 0.1, 'neg': 370.1}\n",
      "{'pos': 0.1, 'neg': 380.1}\n",
      "{'pos': 0.1, 'neg': 390.1}\n",
      "{'pos': 0.1, 'neg': 400.1}\n",
      "{'pos': 0.1, 'neg': 410.1}\n",
      "{'pos': 0.1, 'neg': 420.1}\n",
      "{'pos': 0.1, 'neg': 430.1}\n",
      "{'pos': 0.1, 'neg': 440.1}\n",
      "{'pos': 0.1, 'neg': 450.1}\n",
      "{'pos': 0.1, 'neg': 460.1}\n",
      "{'pos': 0.1, 'neg': 470.1}\n",
      "{'pos': 0.1, 'neg': 480.1}\n",
      "{'pos': 0.1, 'neg': 490.1}\n",
      "{'pos': 0.1, 'neg': 500.1}\n",
      "{'pos': 0.1, 'neg': 510.1}\n",
      "{'pos': 0.1, 'neg': 520.1}\n",
      "{'pos': 0.1, 'neg': 530.1}\n",
      "{'pos': 0.1, 'neg': 540.1}\n",
      "{'pos': 0.1, 'neg': 550.1}\n",
      "{'pos': 0.1, 'neg': 560.1}\n",
      "{'pos': 0.1, 'neg': 570.1}\n",
      "{'pos': 0.1, 'neg': 580.1}\n",
      "{'pos': 0.1, 'neg': 590.1}\n",
      "{'pos': 0.1, 'neg': 600.1}\n",
      "{'pos': 0.1, 'neg': 610.1}\n",
      "{'pos': 0.1, 'neg': 620.1}\n",
      "{'pos': 0.1, 'neg': 630.1}\n",
      "{'pos': 0.1, 'neg': 640.1}\n",
      "{'pos': 0.1, 'neg': 650.1}\n",
      "{'pos': 0.1, 'neg': 660.1}\n",
      "{'pos': 0.1, 'neg': 670.1}\n",
      "{'pos': 0.1, 'neg': 680.1}\n",
      "{'pos': 0.1, 'neg': 690.1}\n",
      "{'pos': 0.1, 'neg': 700.1}\n",
      "{'pos': 0.1, 'neg': 710.1}\n",
      "{'pos': 0.1, 'neg': 720.1}\n",
      "{'pos': 0.1, 'neg': 730.1}\n",
      "{'pos': 0.1, 'neg': 740.1}\n",
      "{'pos': 0.1, 'neg': 750.1}\n",
      "{'pos': 0.1, 'neg': 760.1}\n",
      "{'pos': 0.1, 'neg': 770.1}\n",
      "{'pos': 0.1, 'neg': 780.1}\n",
      "{'pos': 0.1, 'neg': 790.1}\n",
      "{'pos': 0.1, 'neg': 800.1}\n",
      "{'pos': 0.1, 'neg': 810.1}\n",
      "{'pos': 0.1, 'neg': 820.1}\n",
      "{'pos': 0.1, 'neg': 830.1}\n",
      "{'pos': 0.1, 'neg': 840.1}\n",
      "{'pos': 0.1, 'neg': 850.1}\n",
      "{'pos': 0.1, 'neg': 860.1}\n",
      "{'pos': 0.1, 'neg': 870.1}\n",
      "{'pos': 0.1, 'neg': 880.1}\n",
      "{'pos': 0.1, 'neg': 890.1}\n",
      "{'pos': 0.1, 'neg': 900.1}\n",
      "{'pos': 0.1, 'neg': 910.1}\n",
      "{'pos': 0.1, 'neg': 920.1}\n",
      "{'pos': 0.1, 'neg': 930.1}\n",
      "{'pos': 0.1, 'neg': 940.1}\n",
      "{'pos': 0.1, 'neg': 950.1}\n",
      "{'pos': 0.1, 'neg': 960.1}\n",
      "{'pos': 0.1, 'neg': 970.1}\n",
      "{'pos': 0.1, 'neg': 980.1}\n",
      "{'pos': 0.1, 'neg': 990.1}\n",
      "{'pos': 0.1, 'neg': 1000.1}\n",
      "{'pos': 10.1, 'neg': 1000.1}\n",
      "{'pos': 20.1, 'neg': 1000.1}\n",
      "{'pos': 30.1, 'neg': 1000.1}\n",
      "{'pos': 40.1, 'neg': 1000.1}\n",
      "{'pos': 50.1, 'neg': 1000.1}\n",
      "{'pos': 60.1, 'neg': 1000.1}\n",
      "{'pos': 70.1, 'neg': 1000.1}\n",
      "{'pos': 80.1, 'neg': 1000.1}\n",
      "{'pos': 90.1, 'neg': 1000.1}\n",
      "{'pos': 100.1, 'neg': 1000.1}\n",
      "{'pos': 110.1, 'neg': 1000.1}\n",
      "{'pos': 120.1, 'neg': 1000.1}\n",
      "{'pos': 130.1, 'neg': 1000.1}\n",
      "{'pos': 140.1, 'neg': 1000.1}\n",
      "{'pos': 150.1, 'neg': 1000.1}\n",
      "{'pos': 160.1, 'neg': 1000.1}\n",
      "{'pos': 170.1, 'neg': 1000.1}\n",
      "{'pos': 180.1, 'neg': 1000.1}\n",
      "{'pos': 190.1, 'neg': 1000.1}\n",
      "{'pos': 200.1, 'neg': 1000.1}\n",
      "{'pos': 210.1, 'neg': 1000.1}\n",
      "{'pos': 220.1, 'neg': 1000.1}\n",
      "{'pos': 230.1, 'neg': 1000.1}\n",
      "{'pos': 240.1, 'neg': 1000.1}\n",
      "{'pos': 250.1, 'neg': 1000.1}\n",
      "{'pos': 260.1, 'neg': 1000.1}\n",
      "{'pos': 270.1, 'neg': 1000.1}\n",
      "{'pos': 280.1, 'neg': 1000.1}\n",
      "{'pos': 290.1, 'neg': 1000.1}\n",
      "{'pos': 300.1, 'neg': 1000.1}\n",
      "{'pos': 310.1, 'neg': 1000.1}\n",
      "{'pos': 320.1, 'neg': 1000.1}\n",
      "{'pos': 330.1, 'neg': 1000.1}\n",
      "{'pos': 340.1, 'neg': 1000.1}\n",
      "{'pos': 350.1, 'neg': 1000.1}\n",
      "{'pos': 360.1, 'neg': 1000.1}\n",
      "{'pos': 370.1, 'neg': 1000.1}\n",
      "{'pos': 380.1, 'neg': 1000.1}\n",
      "{'pos': 390.1, 'neg': 1000.1}\n",
      "{'pos': 400.1, 'neg': 1000.1}\n",
      "{'pos': 410.1, 'neg': 1000.1}\n",
      "{'pos': 420.1, 'neg': 1000.1}\n",
      "{'pos': 430.1, 'neg': 1000.1}\n",
      "{'pos': 440.1, 'neg': 1000.1}\n",
      "{'pos': 450.1, 'neg': 1000.1}\n",
      "{'pos': 460.1, 'neg': 1000.1}\n",
      "{'pos': 470.1, 'neg': 1000.1}\n",
      "{'pos': 480.1, 'neg': 1000.1}\n",
      "{'pos': 490.1, 'neg': 1000.1}\n",
      "{'pos': 500.1, 'neg': 1000.1}\n",
      "{'pos': 510.1, 'neg': 1000.1}\n",
      "{'pos': 520.1, 'neg': 1000.1}\n",
      "{'pos': 530.1, 'neg': 1000.1}\n",
      "{'pos': 540.1, 'neg': 1000.1}\n",
      "{'pos': 550.1, 'neg': 1000.1}\n",
      "{'pos': 560.1, 'neg': 1000.1}\n",
      "{'pos': 570.1, 'neg': 1000.1}\n",
      "{'pos': 580.1, 'neg': 1000.1}\n",
      "{'pos': 590.1, 'neg': 1000.1}\n",
      "{'pos': 600.1, 'neg': 1000.1}\n",
      "{'pos': 610.1, 'neg': 1000.1}\n",
      "{'pos': 620.1, 'neg': 1000.1}\n",
      "{'pos': 630.1, 'neg': 1000.1}\n",
      "{'pos': 640.1, 'neg': 1000.1}\n",
      "{'pos': 650.1, 'neg': 1000.1}\n",
      "{'pos': 660.1, 'neg': 1000.1}\n",
      "{'pos': 670.1, 'neg': 1000.1}\n",
      "{'pos': 680.1, 'neg': 1000.1}\n",
      "{'pos': 690.1, 'neg': 1000.1}\n",
      "{'pos': 700.1, 'neg': 1000.1}\n",
      "{'pos': 710.1, 'neg': 1000.1}\n",
      "{'pos': 720.1, 'neg': 1000.1}\n",
      "{'pos': 730.1, 'neg': 1000.1}\n",
      "{'pos': 740.1, 'neg': 1000.1}\n",
      "{'pos': 750.1, 'neg': 1000.1}\n",
      "{'pos': 760.1, 'neg': 1000.1}\n",
      "{'pos': 770.1, 'neg': 1000.1}\n",
      "{'pos': 780.1, 'neg': 1000.1}\n",
      "{'pos': 790.1, 'neg': 1000.1}\n",
      "{'pos': 800.1, 'neg': 1000.1}\n",
      "{'pos': 810.1, 'neg': 1000.1}\n",
      "{'pos': 820.1, 'neg': 1000.1}\n",
      "{'pos': 830.1, 'neg': 1000.1}\n",
      "{'pos': 840.1, 'neg': 1000.1}\n",
      "{'pos': 850.1, 'neg': 1000.1}\n",
      "{'pos': 860.1, 'neg': 1000.1}\n",
      "{'pos': 870.1, 'neg': 1000.1}\n",
      "{'pos': 880.1, 'neg': 1000.1}\n",
      "{'pos': 890.1, 'neg': 1000.1}\n",
      "{'pos': 900.1, 'neg': 1000.1}\n",
      "{'pos': 910.1, 'neg': 1000.1}\n",
      "{'pos': 920.1, 'neg': 1000.1}\n",
      "{'pos': 930.1, 'neg': 1000.1}\n",
      "{'pos': 940.1, 'neg': 1000.1}\n",
      "{'pos': 950.1, 'neg': 1000.1}\n",
      "{'pos': 960.1, 'neg': 1000.1}\n",
      "{'pos': 970.1, 'neg': 1000.1}\n",
      "{'pos': 980.1, 'neg': 1000.1}\n",
      "{'pos': 990.1, 'neg': 1000.1}\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for f in movie_reviews.fileids():\n",
    "    if i%10 == 0:\n",
    "        print(model.prior_counts)\n",
    "\n",
    "    label, fileid = f.replace('.txt', '').split('/')\n",
    "    model.fit(movie_reviews.words(fileids = f), label)\n",
    "    nb.fit()\n",
    "    i+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ab0663af-dfc2-4c8d-8163-3dec252e8d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6417810975303053"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_posterior('pos', 'wild mundane'.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "282a3d88-9e80-4c7f-bc52-84a4042b249d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_prior('pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ac87b415-141b-4738-9e7d-a2471390352e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pos': 28.1, 'neg': 115.1}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.ll_counts['terrible']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c94489b5-9dd4-4599-a67c-da242aaee1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "453a2f54-7775-4796-b106-ffb0cc6284f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = movie_reviews.words(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d0b8122-a1ea-4918-9057-3aede132e084",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75465891-0fbc-4b4f-af30-861d0a3288dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.iterate_from()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd7d3ca4-893d-42ca-b32d-eae013aeccff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1583820"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "519cb02b-ded3-4b24-90f7-6b41109be37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4313ed5b-8f74-45ba-bf09-eaa21904c07d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bf12a3-89b5-4374-893f-7bbf00cb18f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(self):\n",
    "    self.dictionary = {}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f96afc3-ec34-4dc8-8aec-beb0ae2622dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
