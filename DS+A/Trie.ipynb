{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b33481-da32-4c77-9778-725bb7506dbe",
   "metadata": {},
   "source": [
    "# Trie\n",
    "## Use a binary tree to store characters. Can be used to quickly search for a word in the trie.\n",
    "## Practical application: Autocomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "427f446b-6550-495a-bd93-df2152afaeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    # We'll use a linked list to store siblings\n",
    "    def __init__(self, val:str):\n",
    "        self.val = val\n",
    "        self.next = None\n",
    "        \n",
    "    def push(self, val:str) -> None:\n",
    "        head = self\n",
    "        while head.next != None:\n",
    "            head = head.next\n",
    "        head.next = Node(val)\n",
    "                \n",
    "    def search(self, val:str):\n",
    "        head = self\n",
    "        while head:\n",
    "            if head.val == val:\n",
    "                return head\n",
    "            else:\n",
    "                head = head.next\n",
    "        return None\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "fda8c605-1836-4704-b2a0-60342635028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Node:\n",
    "#     # We'll use a linked list to store siblings\n",
    "#     def __init__(self, val:str):\n",
    "#         self.item = TrieNode(val)\n",
    "#         self.next = None\n",
    "        \n",
    "#     def push(self, val:str) -> None:\n",
    "#         head = self\n",
    "#         while head.next != None:\n",
    "#             head = head.next\n",
    "#         head.next = Node(val)\n",
    "                \n",
    "#     def search(self, val:str):\n",
    "#         head = self\n",
    "#         while head:\n",
    "#             if head.val == val:\n",
    "#                 return head\n",
    "#             else:\n",
    "#                 head = head.next\n",
    "#         return None\n",
    "            \n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self, val:str) -> None:\n",
    "        self.val = val\n",
    "        self.children = {} # Dictionary of TrieNodes\n",
    "        self.is_word = False\n",
    "        \n",
    "    def insert(self, word:str) -> None:\n",
    "        head = self\n",
    "        for w in word:\n",
    "            children = head.children\n",
    "            if w not in children:\n",
    "                print('Adding ' + w)\n",
    "                head.children[w] = TrieNode(w)\n",
    "            head = children[w]\n",
    "        # mark word as being complete\n",
    "        head.is_word = True\n",
    "            \n",
    "    def find_word(self, word:str) -> bool:\n",
    "        head = self\n",
    "        for w in word:\n",
    "            if w in head.children:\n",
    "                head = head.children[w]\n",
    "            else:\n",
    "                print('Not even close.')\n",
    "                return False\n",
    "        if head.is_word == False:\n",
    "            print('Exists in the trie, but not marked as a complete word')\n",
    "        return head.is_word\n",
    "    \n",
    "    \n",
    "    def dfs(self, node, prefix):\n",
    "        # recursively retrieve all values inorder\n",
    "        if len(node.children) == 0 and node.is_word:\n",
    "            return [prefix + node.val]\n",
    "        if node:\n",
    "            result = []\n",
    "            if node.is_word:\n",
    "                result.extend([prefix + node.val])\n",
    "            for child in node.children:\n",
    "                result.extend(self.dfs(node.children[child], prefix + node.val))\n",
    "            return result\n",
    "                \n",
    "            \n",
    "    \n",
    "    def autocomplete(self, stub:str) -> [str]:\n",
    "        '''\n",
    "        Given a stub, return a list of all potential autocompleted words (must be complete)\n",
    "        '''\n",
    "        \n",
    "        head = self\n",
    "        for w in stub:\n",
    "            if w in head.children:\n",
    "                head = head.children[w]\n",
    "            else:\n",
    "                print('stub does not exist.')\n",
    "                return None\n",
    "        \n",
    "        solutions = self.dfs(head, stub[:-1])\n",
    "        # Now retrieve all descendant nodes, with the stub as a prefix. We'll do this using bfs\n",
    "    \n",
    "        return solutions\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f1184be9-1b2c-43df-88bb-c2cc27acf4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = TrieNode(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "df5b033a-ea99-4c5b-a721-d63f0ae3fcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding b\n",
      "Adding o\n",
      "Adding a\n",
      "Adding s\n",
      "Adding t\n"
     ]
    }
   ],
   "source": [
    "t.insert('boast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ee2d4361-4e0d-4cdd-b92b-926b0f106074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding o\n"
     ]
    }
   ],
   "source": [
    "t.insert('boo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "25cd6ad9-a657-4813-ae9a-479cfd2479ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding m\n"
     ]
    }
   ],
   "source": [
    "t.insert('boom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "56fbd152-52be-44d8-b21e-8db810faeda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding e\n",
      "Adding r\n"
     ]
    }
   ],
   "source": [
    "t.insert('boomer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1a8c5cd6-b41e-4611-bacf-af708c847ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.insert('boast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "83d91804-1472-4e93-b647-76d47bb6c31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.find_word('boo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "390a83f7-a7d6-42e8-9c39-f4c6ee7749a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['boast', 'boo', 'boom', 'boomer']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.autocomplete('bo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cae5b7-349f-4877-bd97-b308a43b955e",
   "metadata": {},
   "source": [
    "### We can make a slight improvement to the autocomplete feature. we can rank order the autocompletes using some counter dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25a8d56-e792-4d60-8ff1-0319b09dd4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b16e3e-f37b-47f4-99a5-2748f26e1cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e9411-f240-4263-8c8e-ec2702343e41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd819a5b-c3df-4b03-a6e1-13159c257d36",
   "metadata": {},
   "source": [
    "# So that's pretty cool. But right now, autocomplete just gives us a laundry list of the items we've marked as being real words (via insert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bcfa70-2940-4d6f-b67c-45fc6dfbca9b",
   "metadata": {},
   "source": [
    "## To really make this interesting, can we build a SMART recommendation tool, that provides autocomplete suggestions based on the previous words in the current sentence?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58fdbba-1fdb-4c71-8fbf-502ad246a6cb",
   "metadata": {},
   "source": [
    "propose a simple ngram model.\n",
    "load up the trie using the training corpus\n",
    "build a multinomial naive bayes classifier, where every complete word has some probability of being the word in question.\n",
    "provide the most likely class (i.e. complete word).\n",
    " whats the point of the trie? \n",
    " right now, the trie only stores individual strings and whether or not their complete words.\n",
    " given a sentence and a partial word, a trie will give us all possible words that partial word could be. i.e. defines the set of possible classes.\n",
    " we can then more narrowly define what the word should be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68ab4af-7959-4cc6-96ad-44afb8c7b2fe",
   "metadata": {},
   "source": [
    "### t = Trie()\n",
    "### data = some corpus\n",
    "### t.load(data)\n",
    "### model = Pipeline(data -> ngram -> tfidf -> multinomialNB)\n",
    "#### Model will be fed sentences. sentences will be broken down and processed. model will try to predict  what the n+1th word is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f3b06635-ae45-4332-8462-1516a2c30125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "90d1ee22-9ba7-40d4-bad4-3950249f5acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8ebe534f-e2e2-4f43-a8db-a7ffd2da7332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.nps_chat import NPSChatCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0ec5555e-4e92-4af0-8331-28ad6b4ea5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = NPSChatCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "17a39434-4698-4d7d-8b09-1d145878188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = reader.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "02a96230-3835-4618-af6f-0a64cb4d21a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fileids() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-165-b78731b2b197>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: fileids() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "reader.fileids()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7fd38d56-a07e-47da-bc0b-4fa599f8879e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function nltk.corpus.reader.nps_chat.NPSChatCorpusReader.posts(self, fileids=None)>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "43d72b85-5612-4f23-944c-89178584f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6cb887c0-ae5a-4482-b1e0-f234853debaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6c53dfbf-b123-41a4-aa55-0f673238e2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "66bd8875-0237-4571-8cea-625abe3c731a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57340"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca84cc7-cf62-498c-b19d-076ce1c21d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dataset from these corpora.\n",
    "# Th eway to do this is to take random sentences, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "4eabc0bf-1df8-4748-befd-501fe34b9ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "def gen_samples_from_sentence(sent):\n",
    "    #words = sent.split(' ')\n",
    "    result = []\n",
    "    for i in range(len(sent)//2):\n",
    "        idx = randint(1, len(sent) - 1)\n",
    "        result.append((sent[:idx], sent[idx]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "f59c78dc-9442-4acc-b9c0-f632e2a4fd97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['this', 'is'], 'the'), (['this'], 'is')]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_samples_from_sentence('this is the best'.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "f8a24284-b20f-42ba-a48c-80b03c3027c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of',\n",
       " \"Atlanta's\",\n",
       " 'recent',\n",
       " 'primary',\n",
       " 'election',\n",
       " 'produced',\n",
       " '``',\n",
       " 'no',\n",
       " 'evidence',\n",
       " \"''\",\n",
       " 'that',\n",
       " 'any',\n",
       " 'irregularities',\n",
       " 'took',\n",
       " 'place',\n",
       " '.']"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92f7145-e2f1-4f28-af61-9225bfb4b3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for sent in brown.sents():\n",
    "    data.append(gen_samples_from_sentence(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a96c2d6-8077-437e-9ce9-d9e0b69cf02d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
